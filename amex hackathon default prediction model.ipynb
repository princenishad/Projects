{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":35332,"databundleVersionId":3723648,"sourceType":"competition"},{"sourceId":3739819,"sourceType":"datasetVersion","datasetId":2231132}],"dockerImageVersionId":30204,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-24T19:05:57.786789Z","iopub.execute_input":"2022-06-24T19:05:57.788093Z","iopub.status.idle":"2022-06-24T19:05:57.796768Z","shell.execute_reply.started":"2022-06-24T19:05:57.788055Z","shell.execute_reply":"2022-06-24T19:05:57.795539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install cupy","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:09:16.404849Z","iopub.execute_input":"2022-06-24T19:09:16.405195Z","iopub.status.idle":"2022-06-24T19:09:27.071230Z","shell.execute_reply.started":"2022-06-24T19:09:16.405165Z","shell.execute_reply":"2022-06-24T19:09:27.070138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy, cudf","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:20:52.333095Z","iopub.execute_input":"2022-06-24T19:20:52.333490Z","iopub.status.idle":"2022-06-24T19:20:53.765798Z","shell.execute_reply.started":"2022-06-24T19:20:52.333452Z","shell.execute_reply":"2022-06-24T19:20:53.764728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:20:57.447331Z","iopub.execute_input":"2022-06-24T19:20:57.448523Z","iopub.status.idle":"2022-06-24T19:20:57.454044Z","shell.execute_reply.started":"2022-06-24T19:20:57.448467Z","shell.execute_reply":"2022-06-24T19:20:57.452769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_file(path = '', usecols = None):\n    # LOAD DATAFRAME\n    if usecols is not None: df = cudf.read_parquet(path, columns=usecols)\n    else: df = cudf.read_parquet(path)\n    # REDUCE DTYPE FOR CUSTOMER AND DATE\n    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    df.S_2 = cudf.to_datetime( df.S_2 )\n    # SORT BY CUSTOMER AND DATE (so agg('last') works correctly)\n    #df = df.sort_values(['customer_ID','S_2'])\n    #df = df.reset_index(drop=True)\n    # FILL NAN\n    df = df.fillna(0) \n    print('shape of data:', df.shape)\n    \n    return df\n\nprint('Reading train data...')\nTRAIN_PATH = '../input/amex-data-integer-dtypes-parquet-format/train.parquet'\ntrain = read_train_file(path = TRAIN_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:21:02.303313Z","iopub.execute_input":"2022-06-24T19:21:02.304121Z","iopub.status.idle":"2022-06-24T19:21:24.433679Z","shell.execute_reply.started":"2022-06-24T19:21:02.304079Z","shell.execute_reply":"2022-06-24T19:21:24.432565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_and_feature_engineer(df):\n    # FEATURE ENGINEERING FROM \n    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n    num_features = [col for col in all_cols if col not in cat_features]\n\n    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n\n    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n\n    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n    del test_num_agg, test_cat_agg\n    gc.collect()\n    print('shape after engineering', df.shape )\n    \n    return df\n\ntrain = process_and_feature_engineer(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:21:49.348715Z","iopub.execute_input":"2022-06-24T19:21:49.349096Z","iopub.status.idle":"2022-06-24T19:21:50.803277Z","shell.execute_reply.started":"2022-06-24T19:21:49.349066Z","shell.execute_reply":"2022-06-24T19:21:50.802236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntargets = targets.set_index('customer_ID')\ntrain = train.merge(targets, left_index=True, right_index=True, how='left')\ntrain.target = train.target.astype('int8')\ndel targets\n\n# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\ntrain = train.sort_index().reset_index()\n\n# FEATURES\nFEATURES = train.columns[1:-1]\nprint(f'There are {len(FEATURES)} features!')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:22:01.697584Z","iopub.execute_input":"2022-06-24T19:22:01.698367Z","iopub.status.idle":"2022-06-24T19:22:03.197437Z","shell.execute_reply.started":"2022-06-24T19:22:01.698327Z","shell.execute_reply":"2022-06-24T19:22:03.196284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_test_file(path = '', usecols = None):\n    if usecols is not None: df = cudf.read_parquet(path, columns=usecols)\n    else: df = cudf.read_parquet(path)\n    # REDUCE DTYPE FOR CUSTOMER AND DATE\n    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    df.S_2 = cudf.to_datetime( df.S_2 )\n    # SORT BY CUSTOMER AND DATE (so agg('last') works correctly)\n    #df = df.sort_values(['customer_ID','S_2'])\n    #df = df.reset_index(drop=True)\n    # FILL NAN\n    df = df.fillna(0) \n    print('shape of data:', df.shape)\n    \n    return df\n\nprint('Reading test data...')\nTest_PATH = '../input/amex-data-integer-dtypes-parquet-format/test.parquet'\ntest = read_test_file(path = Test_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:24:07.817065Z","iopub.execute_input":"2022-06-24T19:24:07.817482Z","iopub.status.idle":"2022-06-24T19:24:48.694133Z","shell.execute_reply.started":"2022-06-24T19:24:07.817439Z","shell.execute_reply":"2022-06-24T19:24:48.693022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gdown","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:34:19.456361Z","iopub.execute_input":"2022-06-24T19:34:19.457401Z","iopub.status.idle":"2022-06-24T19:34:49.211032Z","shell.execute_reply.started":"2022-06-24T19:34:19.457356Z","shell.execute_reply":"2022-06-24T19:34:49.209785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_and_feature_engineer_test(df):\n    # FEATURE ENGINEERING FROM \n    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n    num_features = [col for col in all_cols if col not in cat_features]\n\n    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n\n    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n\n    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n    del test_num_agg, test_cat_agg\n    gc.collect()\n    print('shape after engineering', df.shape )\n    \n    return df\n\ntest = process_and_feature_engineer_test(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:26:12.498170Z","iopub.execute_input":"2022-06-24T19:26:12.498607Z","iopub.status.idle":"2022-06-24T19:26:14.841012Z","shell.execute_reply.started":"2022-06-24T19:26:12.498570Z","shell.execute_reply":"2022-06-24T19:26:14.839969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random\nimport scipy as sp\nimport joblib\nimport itertools\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom itertools import combinations\n\n# ====================================================\n# Configurations\n# ====================================================\n\n#input_dir = '/content/data/'\nseed = 42\nn_folds = 5\ntarget = 'target'\n\n# ====================================================\n# Seed everything\n# ====================================================\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n# ====================================================\n# Read data\n# ====================================================\n#def read_data():\n#    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n#    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n#    return train, test\n\n# ====================================================\n# Amex metric\n# ====================================================\ndef amex_metric(y_true, y_pred):\n    labels = np.transpose(np.array([y_true, y_pred]))\n    labels = labels[labels[:, 1].argsort()[::-1]]\n    weights = np.where(labels[:,0]==0, 20, 1)\n    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n    gini = [0,0]\n    for i in [1,0]:\n        labels = np.transpose(np.array([y_true, y_pred]))\n        labels = labels[labels[:, i].argsort()[::-1]]\n        weight = np.where(labels[:,0]==0, 20, 1)\n        weight_random = np.cumsum(weight / np.sum(weight))\n        total_pos = np.sum(labels[:, 0] *  weight)\n        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n        lorentz = cum_pos_found / total_pos\n        gini[i] = np.sum((lorentz - weight_random) * weight)\n    return 0.5 * (gini[1]/gini[0] + top_four)\n\n# ====================================================\n# LGBM amex metric\n# ====================================================\ndef lgb_amex_metric(y_pred, y_true):\n    y_true = y_true.get_label()\n    return 'amex_metric', amex_metric(y_true, y_pred), True\n\n# ====================================================\n# Train & Evaluate\n# ====================================================\ndef train_and_evaluate(train, test):\n    # Label encode categorical features\n    cat_features = [\n        \"B_30\",\n        \"B_38\",\n        \"D_114\",\n        \"D_116\",\n        \"D_117\",\n        \"D_120\",\n        \"D_126\",\n        \"D_63\",\n        \"D_64\",\n        \"D_66\",\n        \"D_68\"\n    ]\n    cat_features = [f\"{cf}_last\" for cf in cat_features]\n    for cat_col in cat_features:\n        encoder = LabelEncoder()\n        train[cat_col] = encoder.fit_transform(train[cat_col])\n        test[cat_col] = encoder.transform(test[cat_col])\n    # Round last float features to 2 decimal place\n    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n    num_cols = [col for col in num_cols if 'last' in col]\n    for col in num_cols:\n        train[col + '_round2'] = train[col].round(2)\n        test[col + '_round2'] = test[col].round(2)\n    # Get feature list\n    features = [col for col in train.columns if col not in ['customer_ID', target]]\n    params = {\n        'objective': 'binary',\n        'metric': \"binary_logloss\",\n        'boosting': 'dart',\n        'seed': CFG.seed,\n        'num_leaves': 100,\n        'learning_rate': 0.01,\n        'feature_fraction': 0.20,\n        'bagging_freq': 10,\n        'bagging_fraction': 0.50,\n        'n_jobs': -1,\n        'lambda_l2': 2,\n        'min_data_in_leaf': 40\n        }\n    # Create a numpy array to store test predictions\n    test_predictions = np.zeros(len(test))\n    # Create a numpy array to store out of folds predictions\n    oof_predictions = np.zeros(len(train))\n    kfold = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed)\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n        print(' ')\n        print('-'*50)\n        print(f'Training fold {fold} with {len(features)} features...')\n        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n        model = lgb.train(\n            params = params,\n            train_set = lgb_train,\n            num_boost_round = 10500,\n            valid_sets = [lgb_train, lgb_valid],\n            early_stopping_rounds = 100,\n            verbose_eval = 500,\n            feval = lgb_amex_metric\n            )\n        # Save best model\n        joblib.dump(model, f'/content/drive/MyDrive/amex_hackathon/lgbm_fold{fold}_seed{seed}.pkl')\n        # Predict validation\n        val_pred = model.predict(x_val)\n        # Add to out of folds array\n        oof_predictions[val_ind] = val_pred\n        # Predict the test set\n        test_pred = model.predict(test[features])\n        test_predictions += test_pred / n_folds\n        # Compute fold metric\n        score = amex_metric(y_val, val_pred)\n        print(f'Our fold {fold} CV score is {score}')\n        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n        gc.collect()\n    # Compute out of folds metric\n    score = amex_metric(train[target], oof_predictions)\n    print(f'Our out of folds CV score is {score}')\n    # Create a dataframe to store out of folds predictions\n    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[target], 'prediction': oof_predictions})\n    oof_df.to_csv(f'/content/drive/MyDrive/amex_hackathon/oof_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index = False)\n    # Create a dataframe to store test prediction\n    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n    test_df.to_csv(f'/content/drive/MyDrive/amex_hackathon/test_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index = False)\n    \nseed_everything(seed)\n#train, test = read_data()\n#train = cupy.asarray(train)\n#test = cupy.asarray(test)\ntrain = train.as_gpu_matrix()\ntest = test.as_gpu_matrix()\ntrain_and_evaluate(train, test)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T19:46:00.743860Z","iopub.execute_input":"2022-06-24T19:46:00.744281Z","iopub.status.idle":"2022-06-24T19:46:00.804072Z","shell.execute_reply.started":"2022-06-24T19:46:00.744247Z","shell.execute_reply":"2022-06-24T19:46:00.802495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}